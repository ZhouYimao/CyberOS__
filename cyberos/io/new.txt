Digital Life Project: Autonomous 3D Characters with Social Intelligence
Zhongang Cai∗,1,2,3, Jianping Jiang∗,2, Zhongfei Qing∗,2, Xinying Guo∗,1, Mingyuan Zhang∗,1,
Zhengyu Lin2, Haiyi Mei2, Chen Wei2, Ruisi Wang1,2, Wanqi Yin2, Xiangyu Fan2, Han Du2,
Liang Pan1,3, Peng Gao2, Zhitao Yang2, Yang Gao2, Jiaqi Li2, Tianxiang Ren2, Yukun Wei2,
Xiaogang Wang2, Chen Change Loy1, Lei Yang†,2,3, Ziwei Liu†,1
1S-Lab, Nanyang Technological University, 2SenseTime Research, 3Shanghai AI Laboratory
∗Equal Contributions, † Corresponding Author
https://digital-life-project.com
“First Meet”
“Music Lovers”
“Cozy Night”
<speech>We finally meet!
<motion>extends arms
<place>dining table
Acquaintance with 
<speech>Happy to meet you!
<motion>sits upright 
<place>dining table
Acquaintance with
“Shared Interests”
<speech>Oh, it's my new hobby!
<motion>types on the keyboard
<place>desk
Close friend with 
<speech>Cool computer setup!
<motion>takes a step forward
<place>desk
Close friend with 
<speech>Let’s watch a movie.
<motion>suggests thoughtfully
<place>sofa
Best friend with 
<speech>Any weekend plans?
<motion>looks expectantly
<place>sofa
Best friend with 
<speech>Indeed, I love Mozart. 
<motion>raises hand to explain
<place>center
Friend with 
<speech>Are you into art?
<motion>leans forward
<place>center
Friend with
Figure 1. Digital Life Project empowers virtual characters to interact with each other using articulated body motions. We demonstrate the
interaction of two characters across four occasions (episodes) that leads to evolving relationship.
Abstract
In this work, we present Digital Life Project, a frame-
work utilizing language as the universal medium to build
autonomous 3D characters, who are capable of engaging
in social interactions and expressing with articulated body
motions, thereby simulating life in a digital environment.
Our framework comprises two primary components: 1) So-
cioMind: a meticulously crafted digital brain that models
personalities with systematic few-shot exemplars, incorpo-
rates a reflection process based on psychology principles,
and emulates autonomy by initiating dialogue topics; 2)
MoMat-MoGen: a text-driven motion synthesis paradigm
for controlling the character’s digital body. It integrates
motion matching, a proven industry technique to ensure mo-
tion quality, with cutting-edge advancements in motion gen-
eration for diversity.
Extensive experiments demonstrate
that each module achieves state-of-the-art performance in
its respective domain.
Collectively, they enable virtual
characters to initiate and sustain dialogues autonomously,
while evolving their socio-psychological states.
Concur-
rently, these characters can perform contextually relevant
1
arXiv:2312.04547v1  [cs.CV]  7 Dec 2023
bodily movements. Additionally, a motion captioning mod-
ule further allows the virtual character to recognize and ap-
propriately respond to human players’ actions.
1. Introduction
Recent
advancements
in
Large
Language
Models
(LLMs) [73, 82] have transformed the landscape of
human-computer interaction, catalyzing the emergence
of innovative applications across various domains.
Re-
markably, many once far-fetched fantasies have gradually
become tangible realities. In this work, the term Digital
Life Project (DLP), as envisioned in the recent science
fiction blockbuster The Wandering Earth II, is adopted to
frame our endeavor. What qualifies as a digital life? From
the psychological perspective, humans are composed of
internal psychological processes (mind, such as thoughts)
and external behaviors [38]. In this light, our objective is
to harness the sophisticated capabilities of LLM to craft
virtual 3D characters, that emulate the full spectrum of
human psychological processes, and engage in diverse
interactions with synthesized 3D body motions.
Recently, Park et al. introduced Generative Agents [59]
to advance AI agents capable of simulating human-like be-
havior. Despite the encouraging progress, this pioneering
work is built upon many simplification of interaction: the
agents are represented by pixelated 2D figures. Co-LLM-
Agents [97] aims to build collaborative embodied AI and
includes 3D agents. However, the 3D agents are still con-
strained by a small set of actions and do not exhibit the capa-
bility to socialize. Existing works thus overlook the impor-
tance of sophisticated human body language, through which
a crucial amount of information is conveyed [8, 30, 31].
Moreover, there is a notable deficiency in the current model-
ing of social intelligence. This aspect is critical for the cre-
ation of characters that not only mimic human actions but
also possess human-like thinking and emotional responses,
even the ability to foster long-term relationships.
To achieve the aspirations of DLP, we introduce a frame-
work consisting of two essential components. First, the So-
cioMind which is a carefully designed “digital brain”, an-
choring its design in rigorously applied psychological prin-
ciples. Utilizing emergent abilities of LLMs [56, 73, 89],
the brain generates high-level instructions and plans the
character’s behaviors. Notably, SocioMind introduces few-
shot exemplars from psychological tests to form guiding
instructions for personality modeling, utilizes social cog-
nitive psychology theories in the memory reflection pro-
cess, and designs a negotiation mechanism between char-
acters for story progression.
Second, the “digital body”
that introduces the MoMat-MoGen paradigm to address in-
teractive motion synthesis, which exploits the complemen-
tary nature of motion matching [15] and motion genera-
tion [101]. Here, motion matching is a foundational tech-
nique in modern-day industry-level character animation that
retrieves high-quality motion clips from a database to en-
sure motion quality, whereas motion generation is a line of
works that rapidly gained popularity recently for their ex-
cellent ability to produce diverse human motions.
Experiment results demonstrate that SocioMind and
MoMat-MoGen outperform existing arts in their respec-
tive domains. Specifically, SocioMind demonstrates out-
standing alignment between character behavior and psycho-
logical states (e.g., personality and relationship); MoMat-
MoGen is able to achieve a balance between motion qual-
ity and diversity. Equipped with both modules, we further
show DLP’s controllability as manual editing of character
attributes can result in semantically accurate and aestheti-
cally realistic interactive motions. Moreover, we explore
human-character interaction by developing a motion cap-
tioning module that translates monocular human video to
motion description, thus enabling virtual characters to un-
derstand and appropriately respond to human players.
In summary, we contribute 1) DLP, a framework to
build autonomous 3D characters that possess social traits.
It features 2) SocioMind: a controllable psychology-based
“brain” to enable short-term interactive communication and
long-term social evolution; 3) MoMat-MoGen: a “body”
that synthesizes high-quality and diverse interactive mo-
tions through synergizing motion matching and motion gen-
eration.
2. Related Works
2.1. Motion Synthesis
Motion matching is widely employed in the industry to gen-
erate long-lasting, high-quality motion. The classic motion
matching [15] retrieves the segment that best matches the
current pose and target trajectory. Learned motion matching
[36] employs an auto-regressive neural network to predict
the next motion state based on a given control signal. The
Story-to-motion [65] further incorporates semantic control
through LLM and enhances transition using transformer
models.
Recently, significant strides have been made in
motion generative models for text-driven motion genera-
tion. Early works aimed to establish a unified latent space
for natural language and motion sequences [3, 27, 63, 80].
Guo et al. [28], TM2T [29], and T2M-GPT [99] employ
an auto-regressive scheme to generate lengthy motion se-
quences. Diffusion-based generative models have demon-
strated remarkable performance in leading benchmarks for
the text-to-motion task. MotionDiffuse [100], MDM [81],
and FLAME [41] represent early attempts to apply the dif-
fusion model to the text-driven motion generation field.
Subsequent models such as MLD [11], ReMoDiffuse [101],
Fg-T2M [88], and PhysDiff [96] have further advanced this
idea, achieving improved text-motion consistency, motion
2
quality, and physical plausibility.
2.2. LLM Agents
With the emergent abilities of large language models (LLM)
in reasoning, planning, and learning [20, 56, 82, 89], LLMs
swiftly evolve through three phases: the standalone primi-
tive LLM, language agents [2, 37] that directly interact with
the environment via text, and cognitive language agents [60,
72, 86, 93, 94] with internal cognitive structures [78]. Un-
der the prime framework of cognitive language agents, the
system design hinges on the intended application and objec-
tives: reward systems for game agents [86, 91, 105], chains
of API calls for tool agents [61, 72, 76], and so forth. More-
over, the emergence of human-like behaviors in LLMs has
prompted researchers to investigate controllable mental be-
haviors in LLMs, such as a stable personality [70] and hu-
man simulation in political science [5] and social psychol-
ogy [1]. Recently, Social Simulacra [58] and S3 [25] build
agent systems with autonomous posting and reposting skills
in internet community space. Generative Agents [60] fa-
cilitates the formation of social relationships and informa-
tion diffusion by daily schedules and brief communications
within a 2D sandbox gaming space.
3. Methodology
3.1. Text as the Universal Medium
We define behavior, a dictionary-like structured text mes-
sage to carry key information across modules.
For ex-
ample, <speech>Hello!
<motion>waves right hand
<place>table contains pre-set keys encapsulated by pointy
brackets, followed by the respective values, also in natu-
ral language. Behaviors are thus interpretable by the LLM
and the regular-expression parser. In this work, we focus
on <motion>, but we discuss the use of other tokens in the
Supplementary Material: <place> triggers navigation and
basic state transfer (e.g., sit down), <speech> may be used
for face control.
3.2. Active-Passive Mechanism
There exists an intrinsic order in human interaction. For ex-
ample, “shaking hands” may appear to be a simultaneous
action by two subjects, it typically initiates with one per-
son extending a hand first. Moreover, the other person’s
action is largely predictable: it is socially appropriate for
that person to reciprocate the handshake as a basic cour-
tesy. Drawing from this observation, we design the Active-
Passive Mechanism shown in Fig. 2, where the subject to
whom the behavior is assigned becomes the “active” char-
acter, whereas the partner becomes the “passive” character.
The active character generates a motion pair for both parties
engaged in the interaction, and the passive behavior corre-
sponds to the passive motion. Both passive behavior and
motion are then passed to the passive character. However,
the passive character still retains discretion: it only executes
the passive motion if its brain “approves” the passive behav-
ior (by prompting the LLM with the suggested behavior and
behavior context in its memory). Note that the “active” and
“passive” roles constantly swap between characters as the
interaction progresses.
3.3. Interactive Motion Synthesis
In our application scenario, the generated actions need to
fulfill two main requirements: 1) They must be highly ac-
curate to ensure natural interaction between characters, such
as having sufficient contact when shaking hands. 2) They
should generate diverse actions to adapt to different plots.
In the literature, there are two common solutions for motion
synthesis: motion matching and motion generation. How-
ever, each of these methods alone can only meet one of our
requirements. Therefore, in this paper, we propose a new
method called MoMat-MoGen to combine the strengths of
both, generating dual-person actions that are both diverse
and accurate. As shown in Fig. 2, MoMat-MoGen lever-
ages motion matching (Sec. 3.3.1) to achieve a relevant (but
not necessarily perfect) motion from a small database as a
prior, and motion generation (Sec. 3.3.2) afterward to diver-
sify the motion with text input while retaining interactive
relations between two characters.
3.3.1
Motion Matching for High-Quality Motion Prior
The motion matching algorithms retrieve motion segments
from a database in an auto-regressive manner based on pre-
defined features. The basic motion matching [15] relies on
state-based features (e.g., joint position) along with trajec-
tory.
The story-to-motion [65] further incorporates text-
based features to enable semantic control. However, both
methods are designed for single-person scenarios.
In this work, we extend the Text-based Motion Match-
ing [65] to accommodate interactive scenarios. Our objec-
tive is to find a pair of motion clips for both characters that
align with the query text and trajectory while maintaining
a consistent body pose to ensure coherence with the pre-
vious motion. In this light, we use a coarse-to-fine mo-
tion search strategy, leveraging the text for a high-level se-
mantic understanding of the desired motion, and kinematic
features for the low-level control.
First, we incorporate
semantic control by employing a pre-trained sentence en-
coder [47] to extract text embedding from the query text.
Then top-K1 candidates are selected using cosine similar-
ity for subsequent matching. Second, trajectory and coher-
ence constraints are incorporated through joint kinematics
features. For the trajectory constraint, the features include
the position of the hip joint and the facing direction. For
the coherence constraint, the features include positions, ve-
3
SocialMind
SocialMind
Active Behavior
<speech>Yes! Bug fixed!
<motion>high five 
<place>desk
Retrieved Passive Behavior
<motion>high five
Passive Behavior
<motion>high five
Active Character
Passive Character
Active Behavior
<motion>high five
Block  × n
Embedding
Layer
DSMA
Stylization
Block
⨁
FFN
Stylization
Block
⨁
𝑝𝑝𝑥𝑥
𝑝𝑝𝑦𝑦
𝑟𝑟
𝑥𝑥
𝑟𝑟
𝑦𝑦
𝑡𝑡, 𝐱𝐱𝑡𝑡, 𝐲𝐲𝑡𝑡
𝑡𝑡−1, 𝐱𝐱𝑡𝑡−1, 𝐲𝐲𝑡𝑡−1
0, 𝐱𝐱0, 𝐲𝐲0
⋯
𝑇𝑇, 𝐱𝐱𝑇𝑇~𝒩𝒩(0, 𝑰𝑰), 𝐲𝐲𝑇𝑇~𝒩𝒩(0, 𝑰𝑰)
⋯
M𝑥𝑥, M𝑦𝑦
Learnable
Layers
P
𝑥𝑥, P
𝑦𝑦
Learnable
Layers
Text-based 
Retrieval
Joint Kinematics-
based Retrieval
Retrieved Motion Pair
Synthesized Motion Pair (Variant N)
Synthesized Motion Pair (Variant 1)
MoMat-MoGen
Hip Position
Facing Direction
Joint Positions
Joint Rotations
Joint Velocities
…
“high five”
“high five”
Motion Database
⋯
Element-wise Addition
Text Encoding
Reference Encoding
⨁
Timestep Encoding
Denoising Step
𝑡𝑡
𝑡𝑡
e  
Passive Motion
CLIP
Active Motion
MoMat-MoGen
“hip bump”
“head pat”
“hand kiss”
“shoulder massage”
“high five”
“high five”
Figure 2. Digital Life Project framework for interactive autonomous characters. The top left part depicts the Active-Passive Mechanism,
and the rest of the figure illustrates MoMat-MoGen. SocioMind is shown in details in Fig. 3.
locities, and rotation in 6D space [103] of the body joints.
For the two-person scenario, a new challenge arises: the
interaction between the two characters requires that their
relative positions and orientations align with the intended
motions. Therefore, the relative position of the other char-
acter is taken into account to minimize blending artifacts
caused by long-distance movements. To expedite retrieval,
the aforementioned features are pre-calculated and Z-Score
normalization is applied to account for magnitude differ-
ences. During retrieval, query features are calculated based
on the current pose and target trajectory, and the Top-K2
motions are selected using the Euclidean distance. Random
selection is used if multiple suitable candidates exist.
Moreover, the neural motion blending model is used [65]
to generate the transition motion. Hence, the short motion
clips are blended into long motions. Notably, the blending
model provides smooth transitions to let the character move
to the correct place and turn in the correct direction to inter-
act with the other character. Furthermore, motion matching
is used for single-person motions. This includes 1) naviga-
tion in the scene, where multi-agent path finder [75] is used
to plan a collision-free trajectory, follow which walking mo-
tions are matched from AMASS, and 2) basic character-
object interaction such as “sit down on the chair”. More
details are included in the Supplementary Material.
3.3.2
Motion Generation for Diversity
The MoMat-MoGen structure shares many similarities with
ReMoDiffuse [101], incorporating retrieval techniques to
enhance generation quality. Applying ReMoDiffuse to in-
teraction generation is not trivial. Firstly, it lacks a mech-
anism for interaction modeling, resulting in a poor corre-
lation between the two generated sequences.
Secondly,
achieving physical naturalness is challenging if we solely
rely on data-driven generation. To address these challenges,
we 1) design a Dual-path Semantic-Modulated Attention
module (DSMA) to model the interaction between two indi-
viduals. 2) During the inference stage, we adaptively extract
interaction information from the referenced motion and use
it as a constraint for the denoising process, providing addi-
tional supervisory signals.
Motion Diffusion Model. In the diffusion process, it re-
peatedly adds Gaussian noises to the clean motion sequence
pair (x0, y0) to noised sequence pair (xT , yT ).
q(xT , yT |x0, y0) :=
T
Y
t=1
q(xt, yt|xt−1, yt−1),
q(xt, yt|xt−1, yt−1) := N(
p
1 −βt(xt−1, yt−1), βtI),
(1)
where T is the total diffusion steps. β1, · · · , βT is a series
of pre-defined variance scales for different timesteps. In
the reverse process, given the text prompt P, the motion
matching result ¯
Θ and the timestep t, the initial sequence
4
LLM
Short-Term Interactive Communication
Prompt to LLM
Assume you are a person named Xiaotao.
You possess such psychological states:
Personality: lively and sensitive, with high agreeableness.
Relationship: new friend with medium intimacy.
Current episode backgrounds and topics are:
Zhixu see Xiaotao using a computer.
Recent AGI progress; Digital life; Coding
Behavior contexts in this conversation are:
BEHAVIOR CONTEXTS IN THIS EPISODE
In memories, you have such relevant events and thoughts:
Zhixu showed welcome when we first met.
I’m an autonomous digital avatar.
Psychological research has found such principles:
A person with high agreeableness tends to behave/think: 
think of others first and be friendly to them.
Now respond based on the information above:
<Speech> Oh! CODE ERROR! 
<Motion> Point at screen
<Place> Desk
<Speech> Undefined variable?
<Motion> Hand on cheek
<Place> Desk
Background     : Shared Interests
<Speech> Yes! Bug fixed!
<Motion> High five
<Place> Desk
Long-Term Social Evolution
Psychological Reflection
Summarize Events
Update 
Psychological
States
Summarize Thoughts
Propose Topics
Generate Backgrounds
of Next Episode  
Planning with  Topic Proposal
Weekend Plan
Poignancy: 6
Emergency: 5
Assume you are a psychologist:
One face such things recently:
NEW EVENTS, THOUGHTS
Their previous relationship are:
New friend with medium intimacy
Based on PYSCHOLOGY 
THEORIES, new relationship is:
{Relationship: Close friends,
Intimacy: 7}
Background Selection
for Next Episode
Background     : Cozy Night
Persona Instruction 
Database 
Core Pathway
Retrieved Data
Module Details 
Prompt
SocioMind Architecture
Assume you are a person with:
UPDATED PSYCHOLOGICAL STATES 
You face such things recently:
NEW EVENTS, THOUGHTS
Previous storylines are:
PAST EPISODE BACKGROUND
So the topics you want to start is:
[{Topic: Movie “Mountains may depart”,
Poigancy: 6,     Emergency: 5}, …]
Figure 3. Overview of SocioMind. To enable 3D characters with social intelligence, our brain utilizes psychological principles to emulate
controllable behaviors for short-term interactive communication. For long-term social evolution, our brain assures the consistency of
psychological states and plots towards initial settings through psychological reflection and planning with topic proposal.
pair is estimated by a network Sθ(xt, yt, t, ¯
Θ, P).
Network Architecture. Similar to ReMoDiffuse, our net-
work is built upon transformer layers. We modify the de-
sign of the attention module in ReMoDiffuse to better cap-
ture the interaction. Specifically, in our DSMA module, the
input includes motion feature sequences, fx and fy, fea-
ture sequences extracted from the motion matching results,
rx and ry, and text feature sequences px and py. When
refining fx, we utilize the generated global attention from
fx, fy, rx, px. The process is similar when refining fy. This
approach ensures a more comprehensive fusion of text in-
formation, interaction states, and prior information from
motion matching.
Training and Inference. In the training stage, we only use
the reconstruction loss as the target:
L = MSE((x0, y0), Sθ(xt, yt, t, ¯
Θ, P)).
(2)
In the inference stage, we introduce a contact loss to make
the interaction part more natural.
¯
S = S + λ · ∇(
X
i,j1,j2
∥¯
Di,j1,j2 −Di,j1,j2∥· [ ¯
Di,j1,j2 < γ]),
(3)
where ¯
Di,j1,j2 indicate the distance between the j1-th joint
and the j2-th joint in the i-th frame from the motion match-
ing results. Di,j1,j2 is the distance from the motion gen-
eration results. [·] is the Iverson bracket whose value is 1
if and only if the expression inside the parentheses is true.
Otherwise the value will be 0. This auxiliary loss enforces
the generated results to imitate the interaction pattern from
the prior information and will yield more natural motions.
3.4. Controllable Emulation of Human Psychology
From a social psychology perspective, human social intelli-
gence is characterized by 1) various and patterned interac-
tive behaviors during short-term communication [8], and 2)
the evolution of emotions, attitudes, and relationships etc.
over long-term interactions [14, 52, 67]. Hence, we pro-
pose SocioMind, a text-centric cognitive framework derived
from the idea of “from strings to symbolic AGI” [53, 78].
As shown in Fig. 3, when avatars conduct short-term com-
munication, SocioMind takes the text interpretation results
as input. Guided by psychological states, persona instruc-
tions, relevant memories, and context behaviors, SocioMind
outputs behavior to manipulate the 3D character. For long-
term social evolution, SocioMind autonomously reflects on
psychological states at the end of each interaction session,
where several rounds of behaviors are generated between
characters. We refer to such a session as a episode. It also
determines the background for the next episode through
planning with topic proposal.
3.4.1
Short-Term Interactive Communication
Short-term interactive communication means interactive be-
haviors in multiple dimensions, which are strongly influ-
enced by internal psychological states. Here we adopt the
most critical dimensions with psychological theories: Big
Five Trait model [40] for personality, long-term and short-
5
term motivations [83], central beliefs [34], and trust [67,
69], intimacy [52], and supportiveness [16, 17] in social
relationships.
However, the safe alignment enables cur-
rent LLMs with a predefined personality, such as a friendly
and cooperative disposition [56, 70, 82]. To enhance the
controllability of psychological states on behaviors, we in-
troduce persona instructions: few-shot exemplars from a
reverse-engineering approach on open psychological tests.
Persona instructions.
In CoT [90], constructing accurate
few-shot exemplars can effectively enhance the reasoning
capability of LLMs. When prompting LLMs to infer be-
havior based on human psychological states, crafting pre-
cise and reliable exemplars presents a challenging task due
to the lack of an exemplar database with high quality. Con-
sidering that lots of psychological tests [18, 24, 32, 51]
measure psychological traits through observable behavior,
we adopt a reverse-engineering method to build a database
of trait-to-behavior relationships from psychological tests.
For psychological tests, we choose International Person-
ality Item Pool (IPIP) [19, 21, 77], an open-sourced tool
with over 3,000 items and 250 scales for creating advanced
measures of personality, motivations, and etc. Each item,
called persona instruction, in this database follows the for-
mat: “A person with {extend} {trait dimension} tends to
behave/think: {behavior}”, where {extend} are “high” or
“low” according to the test questionnaire setup. For interac-
tive behavior generation, we retrieve the most similar per-
sona instructions by text embeddings to obtain few-shot ex-
emplars.
3.4.2
Long-Term Social Evolution
Long-term social intelligence requires consistency in two
aspects with the initial character setup: 1) the evolution of
psychological states such as emotions, relationships, and
motivations etc. towards others [14, 16, 52, 67]; 2) the pro-
gression of overall plots or events [4]. SocioMind achieves
the former aspect through psychological reflection and the
latter aspect through planning with the topic proposal.
Psychological Reflection.
Theories in social cognitive
psychology [7, 13, 33] suggest that humans learn, attribute,
and form judgments about others from past experiences.
Therefore, we introduce a reflection mechanism based on
psychological principles. Within each episode, agents in-
trospect on their emotions periodically. At the end of each
episode, agents summarize events and their thoughts into a
memory system based on the behavior contexts. Events rep-
resent occurrences or facts perceived by the agent, whereas
thoughts are ideas, musings, or attitudes generated by
the agent based on their personality and past experiences.
Leveraging current events and thoughts, agents retrieve past
relevant events and thoughts, and reflect on their motives,
central beliefs, and social relationships. For instance, after
‘knowing they share the same interests’, the intimacy of two
characters will increase with psychological reflection.
Planning with Topic Proposal.
We create a planning
module with a topic proposal mechanism for diverse and
plausible story progression. After each psychological re-
flection, an agent independently proposes new topics for the
next episode based on past memories and character settings.
Each agent then proposes the background and initial states
of both agents for the upcoming episode based on the pro-
posed topics. The two agents collect the topics proposed by
them and select the most important one for the next episode.
Through this methodology, the two agents can continuously
interact with each other from one episode to the next. For
example, after the topic proposal, the character wants to
start several topics (such as the movie ‘Mountains may de-
part’ with the highest emergency and poignancy) and gen-
erate the background ‘Weekend Plan’ for next episode. The
two characters, based on the proposals offered by each, will
select an option that holds both high priority and signifi-
cance, forming the background for the subsequent episode.
3.5. Motion Captioning
We design a motion-to-text translation module inspired by
the success of instruction tuning in multi-modal models
such as Otter [42], implemented with a retrieval-augmented
motion encoder similar to that in ReMoDiffuse [101], with
specialized modifications in its attention module to amplify
motion feature extraction via multimodal data integration.
This encoder is seamlessly integrated with the pretrained
MPT-1B RedPajama language model, aligning with struc-
tural paradigms in the text-vision domain. Due to space
constraints, we include an illustration of the module and
more details in the Supplementary Material.
4. Experiments
To the best of our knowledge, Digital Life Project is the
first comprehensive framework to enable autonomous so-
cial characters with articulated 3D bodies. Hence, we break
down Digital Life Project into its modules for a fair com-
parison with existing arts.
4.1. Datasets
DLP-MoCap.
To establish a high-quality dataset of se-
mantic interactive motions, we used an optical MoCap sys-
tem consisting of 30 cameras to capture body motions and
inertial sensor-based MoCap gloves to track hand motions.
We hired four professional actors/actresses (two male and
two female) for more expressive motions. We processed the
MoCap data into SMPL-X [62] format by SOMA [26] on
6
Table 1. Interactive Motion Synthesis results on the DLP test set. ‘↑’(‘↓’) indicates that the values are better if the metric is larger
(smaller). We run all the evaluations 20 times and report the average metric and 95% confidence interval is. The best result are in bold and
the second best result are underlined. Our MoMat-MoGen method achieves the best balance between accuracy and diversity.
Methods
R Precision↑
FID↓
MM Dist↓
Diversity↑
MultiModality↑
Top 1
Top 2
Top 3
Real motions
0.541±.002
0.758±.002
0.850±.002
0.000±.000
3.430±.012
4.207±.071
-
MotionDiffuse [100]
0.035±.004
0.058±.005
0.098±.007
14.883±.824
4.199±.21
0.677±.018
0.655±.018
ReMoDiffuse [101]
0.425±.002
0.627±.003
0.773±.003
0.131±.004
3.582±.015
4.097±.052
0.472±.009
InterGen [45]
0.403±.003
0.582±.003
0.728±.003
0.082±.002
3.615±.014
4.186±.048
0.728±.021
Ours (MoMat Only)
0.517±.001
0.652±.001
0.802±.001
0.034±.000
3.313±.001
0.332±.001
0.002±.000
Ours (MoMat-MoGen)
0.495±.003
0.651±.004
0.792±.004
0.071±.002
3.561±.017
4.025±.050
0.452±.012
Table 2. Motion Captioning results on the HumanML3D test set. Our evaluation methodology aligns with the TM2T [29] metrics, but
we uniquely utilize unprocessed ground truth texts for calculating linguistic metrics as done in MotionGPT [39].
Methods
R Precision↑
MMDist ↓
CIDEr ↑
Blue@1 ↑
Blue@4 ↑
Rouge ↑
BertScore ↑
Top 1
Top 3
Real motions
0.523
0.828
2.901
−
−
−
−
−
TM2T [29]
0.516
0.823
2.935
16.8
48.9
7.00
38.1
32.2
MotionGPT [39]
0.543
0.827
2.821
29.2
48.2
12.5
37.4
32.4
Ours
0.551
0.832
2.813
36.2
51.1
15.5
41.9
35.0
Plot Consistency
Psychological State Consistency
2
4
6
Scores
Generative Agents
w\o Persona Instruction
w\o Topic Proposal
w\o Psychological Reflection
Full Architecture
Figure 4. Ablation results on consistency with 95% confidence.
body parameters and MANO [68] mapping for hand param-
eters. For accurate semantic labeling on motions, we em-
ployed 10 human annotators to label semantic tags and cor-
responding motion segments based on on-site demonstra-
tion videos and MoCap data. DLP-MoCap comprises basic
motions (22%), two-person interaction motions (56%), and
short script plays (22%), giving rise to over nine hours of
human motion at 30 FPS and over 4K text-to-motion pairs.
More datasets.
Besides DLP-MoCap, we also evaluated
the proposed MoMat-MoGen module on InterHuman [45],
another public interactive motion dataset. Furthermore, we
evaluated the proposed motion captioning module using
two key public motion-text datasets: KIT-ML [64] and Hu-
manML3D [28]. Due to space constraint, we include the
test results on KIT-ML and InterHuman in the Supplemen-
tary Material.
4.2. Interactive Motion Synthesis
Table 1 presents a comparative analysis of our proposed
interactive motion generation method against three exist-
ing approaches: ReMoDiffuse [101], MotionDiffuse [100],
and InterGen [45]. Our both methods exhibit significant
Central Belief
Motivation
Personality
Relationship
0.0
0.2
0.4
0.6
0.8
Accuracy
Generative Agents
SocioMind
Figure 5. Results on controllibility with 95% confidence.
improvements on the DLP dataset, especially in R preci-
sion, FID, and MM Dist metrics. Moreover, our MoMat-
MoGen version achieves competitive results in terms of di-
versity and multi-modality. Moreover, we have achieved an
impressive balance between precision and diversity, which
is essential for our application, ensuring that the generated
motions closely align with well-matched motion references
that possess strong priors while being varied and diverse.
4.3. Social Intelligence
To evaluate the social intelligence of SocioMind, we mea-
sure the controllability of behaviors in short-term interactive
communication and the consistency of psychological states
and plots in long-term social evolution. Following the pre-
vious evaluation approach [60], we engage 47 human eval-
uators (ranging in age from 20 to 45 years old) to review the
behavioral records of the agents. More details are included
in the Supplementary Material.
4.3.1
Controllability
Controllability is measured by whether altering psycho-
logical traits can cause noticeable different behaviors in
short-term communication. We show evaluators the behav-
7
<motion>pats on shoulder
<place>center
<motion>gets patted
<place>center
Episode Background: Xiaotao is feeling sad lately.
Couples
Story Progression
<motion>hugs
<place>center
<motion>rests head
<place>sofa
<motion>extends arm
<place>sofa
<motion>hugs
<place>center
Story Progression
Friends
<motion>raises fist
<place>center
<motion>shows encouragement
<place>center
Figure 6. We explore the controllability of DLP. Given the same background, manually editing the relationship state between characters,
results in different social behaviors. Interestingly, “couples” tend to have more intimate interactions than “friends”. The crown indicates the
active player. The story progression bar is color-coded in accordance with the stages represented by boxes: gray boxes represent behaviors,
whereas yellow boxes represent active-passive swapping in between behaviors.
Figure 7. Our motion captioning module translates human motion into text description, allowing a virtual character to respond to the human
player’s “fist bump”. Top Left: RGB video of the human player; Bottom Left: motion capture [10] result; Top right: first-person view of
the human-driven character; Bottom right: third-person view of the interaction. More details are included in the Supplementary Material.
ioral records of 64 episodes, ask them to select the cor-
responding psychological traits from the provided options,
and subsequently calculate the accuracy. Results in Fig. 5
show that SocioMind significantly outperforms Generative
Agents [60] in key attributes: central belief, motivation, per-
sonality, and relationship, demonstrating the effective guid-
ance of persona instructions for the LLM in simulating in-
teractive human behavior.
4.3.2
Consistency
Long-term social evolution consistency implies that the plot
development and internal state changes are coherent with
initial settings. To measure this, we use four different types
of initial settings (family, crime, romance, and military) to
generate records with multiple episodes. Human evaluators
use the records to rate the degrees of consistency on plots
and psychological states on a scale of 1 to 9. Thus we evalu-
ate the effectiveness of modules in the SocioMind for social
evolution. Results in Fig. 4 show that SocioMind demon-
strates superior performance over Generative Agents [60]
on consistency over plots and psychological states, and ab-
lating results show that persona instruction, psychological
reflection, and planning with topic proposal are crucial for
long-term social evolution.
4.4. Motion Captioning
Our experiments, following TM2T [29], reveal that our
model significantly outperforms existing methods, partic-
ularly in linguistic metrics, aligning with our design objec-
tives, the high-quality and precise captioning.
4.5. Visualization
As shown in Fig. 6, our framework possesses a rational
correlation between psychological states and physical be-
haviors. In addition, our system has the potential to add hu-
man players in the virtual world to interact with the digital
avatars (Fig. 7).
8
5. Conclusion
In this paper, we introduce Digital Life Project, an innova-
tive and holistic system that harnesses the latest advance-
ments in generative models to create autonomous 3D char-
acters. DLP integrates SocioMind, a text-centric cognitive
framework that simulates sophisticated internal psycholog-
ical processes, and MoMat-MoGen, a text-driven motion
synthesis pipeline that replicates diverse external physical
behaviors.
Both modules achieve state-of-the-art perfor-
mance in the respective domains, enabling the entire system
to engage in natural interactions with social intelligence.
Acknowledgement
We extend our sincere gratitude to Fei Wang, Jingyi Jiang,
Jinyun Lyu, Wanying Zu, Bo Li, Yuanhan Zhang, Jiawei
Ren, and Cunjun Yu for their insights and assistance in the
areas of multi-modal models, psychological processes, aes-
thetics, and the overall framework.
References
[1] Gati V Aher, Rosa I Arriaga, and Adam Tauman Kalai. Us-
ing large language models to simulate multiple humans and
replicate human subject studies. In International Confer-
ence on Machine Learning, pages 337–371. PMLR, 2023.
3
[2] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen
Chebotar, Omar Cortes, Byron David, Chelsea Finn,
Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman,
et al.
Do as i can, not as i say: Grounding language
in robotic affordances. arXiv preprint arXiv:2204.01691,
2022. 3
[3] Chaitanya Ahuja and Louis-Philippe Morency.
Lan-
guage2pose: Natural language grounded pose forecasting.
In 2019 International Conference on 3D Vision (3DV),
pages 719–728. IEEE, 2019. 2
[4] Irwin Altman and Dalmas A Taylor. Social penetration:
The development of interpersonal relationships. Holt, Rine-
hart & Winston, 1973. 6, 17, 25, 27
[5] Lisa P Argyle, Ethan C Busby, Nancy Fulda, Joshua R
Gubler, Christopher Rytting, and David Wingate. Out of
one, many: Using language models to simulate human sam-
ples. Political Analysis, 2023. 3
[6] Lee Averell and Andrew Heathcote. The form of the for-
getting curve and the fate of memories. Journal of mathe-
matical psychology, 2011. 16
[7] Albert Bandura and Richard H Walters.
Social learning
theory. Englewood cliffs Prentice Hall, 1977. 6
[8] Charles R Berger, Michael E Roloff, and David R Ewold-
sen. The handbook of communication science. Sage, 2010.
2, 5
[9] Randy L Buckner, Jessica R Andrews-Hanna, and Daniel L
Schacter. The brain’s default network: anatomy, function,
and relevance to disease. Annals of the new York Academy
of Sciences, 2008. 15
[10] Zhongang Cai, Wanqi Yin, Ailing Zeng, Chen Wei,
Qingping Sun, Yanjun Wang, Hui En Pang, Haiyi Mei,
Mingyuan Zhang, Lei Zhang, et al.
Smpler-x: Scaling
up expressive human pose and shape estimation.
arXiv
preprint arXiv:2309.17448, 2023. 8, 19
[11] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao
Chen, and Gang Yu. Executing your commands via motion
diffusion in latent space. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 18000–18010, 2023. 2
[12] Lin Chin-Yew. Rouge: A package for automatic evalua-
tion of summaries. In Proceedings of the Workshop on Text
Summarization Branches Out, 2004, 2004. 24
[13] Chi-yue Chiu, Ying-yi Hong, and Carol S Dweck. Lay dis-
positionism and implicit theories of personality. Journal of
personality and social psychology, 73(1):19, 1997. 6
[14] Robert B Cialdini and Noah J Goldstein. Social influence:
Compliance and conformity. Annu. Rev. Psychol., 2004. 5,
6, 15, 17
[15] Simon Clavet. Motion matching and the road to next-gen
animation. In Proc. of GDC, 2016. 2, 3
[16] Sheldon Cohen. Social relationships and health. American
psychologist, 2004. 6, 15, 17
[17] Sheldon Cohen and Thomas A Wills. Stress, social support,
and the buffering hypothesis. Psychological bulletin, 1985.
6, 15
[18] Paul T Costa and Robert R McCrae. The revised neo per-
sonality inventory (neo-pi-r). The SAGE handbook of per-
sonality theory and assessment, pages 179–198, 2008. 6
[19] Colin G DeYoung, Lena C Quilty, and Jordan B Peterson.
Between facets and domains: 10 aspects of the big five.
Journal of personality and social psychology, 2007. 6, 16
[20] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong
Qiu, Zhilin Yang, and Jie Tang. Glm: General language
model pretraining with autoregressive blank infilling. arXiv
preprint arXiv:2103.10360, 2021. 3
[21] Graham A du Plessis and Gideon P de Bruin.
Using
rasch modelling to examine the international personality
item pool (ipip) values in action (via) measure of charac-
ter strengths. Journal of Psychology in Africa, 2015. 6,
16
[22] Starkey Duncan Jr. Nonverbal communication. Psycholog-
ical Bulletin, 1969. 15
[23] Hermann Ebbinghaus. Memory: A contribution to experi-
mental psychology. Annals of neurosciences, 2013. 16
[24] Hans Jurgen Eysenck and Sybil Bianca Giuletta Eysenck.
Manual of the Eysenck Personality Questionnaire (junior
& adult). Hodder and Stoughton Educational, 1975. 6
[25] Chen Gao, Xiaochong Lan, Zhihong Lu, Jinzhu Mao,
Jinghua Piao, Huandong Wang, Depeng Jin, and Yong
Li.
S3:
Social-network simulation system with large
language model-empowered agents.
arXiv preprint
arXiv:2307.14984, 2023. 3
[26] Nima Ghorbani and Michael J. Black.
SOMA: Solving
optical marker-based mocap automatically.
In Proc. In-
ternational Conference on Computer Vision (ICCV), pages
11117–11126, 2021. 6, 21
9
[27] Anindita Ghosh, Noshaba Cheema, Cennet Oguz, Christian
Theobalt, and Philipp Slusallek. Synthesis of compositional
animations from textual descriptions. In Proceedings of the
IEEE/CVF International Conference on Computer Vision,
pages 1396–1406, 2021. 2
[28] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji,
Xingyu Li, and Li Cheng. Generating diverse and natural 3d
human motions from text. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 5152–5161, 2022. 2, 7, 19, 23, 24, 25
[29] Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. Tm2t:
Stochastic and tokenized modeling for the reciprocal gen-
eration of 3d human motions and texts, 2022. 2, 7, 8, 19,
24, 26
[30] Edward Twitchell Hall. The hidden dimension. Anchor,
1966. 2
[31] Edward T Hall. The silent language. Anchor, 1973. 2
[32] Starke R Hathaway and John C McKinley.
A multipha-
sic personality schedule (minnesota): I. construction of the
schedule. The Journal of Psychology, 1940. 6
[33] Fritz Heider.
The psychology of interpersonal relations.
Psychology Press, 2013. 6
[34] E Tory Higgins. Self-discrepancy: a theory relating self and
affect. Psychological review, 1987. 6, 15
[35] Daniel Holden, Taku Komura, and Jun Saito.
Phase-
functioned neural networks for character control.
ACM
Transactions on Graphics (TOG), 36(4):1–13, 2017. 23
[36] Daniel Holden, Oussama Kanoun, Maksym Perepichka,
and Tiberiu Popa. Learned motion matching. ACM TOG,
39(4):53–1, 2020. 2, 23
[37] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky
Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor
Mordatch, Yevgen Chebotar, et al. Inner monologue: Em-
bodied reasoning through planning with language models.
arXiv preprint arXiv:2207.05608, 2022. 3
[38] William James. The principles of psychology. Cosimo, Inc.,
2007. 2, 15, 18
[39] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and
Tao Chen.
Motiongpt: Human motion as a foreign lan-
guage. arXiv preprint arXiv:2306.14795, 2023. 7, 19, 26
[40] Oliver P John, Sanjay Srivastava, et al. The big-five trait
taxonomy: History, measurement, and theoretical perspec-
tives. 1999. 5, 15
[41] Jihoon Kim, Jiseob Kim, and Sungjoon Choi. Flame: Free-
form language-based motion synthesis & editing. In Pro-
ceedings of the AAAI Conference on Artificial Intelligence,
pages 8255–8263, 2023. 2
[42] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,
Jingkang Yang, and Ziwei Liu.
Otter: A multi-modal
model with in-context instruction tuning.
arXiv preprint
arXiv:2305.03726, 2023. 6, 19, 20, 24
[43] Jiefeng Li, Siyuan Bian, Chao Xu, Zhicun Chen, Lixin
Yang, and Cewu Lu. Hybrik-x: Hybrid analytical-neural
inverse kinematics for whole-body mesh recovery. arXiv
preprint arXiv:2304.05690, 2023. 19
[44] Zhihao Li, Jianzhuang Liu, Zhensong Zhang, Songcen Xu,
and Youliang Yan.
Cliff: Carrying location information
in full frames into human pose and shape estimation. In
European Conference on Computer Vision, pages 590–606.
Springer, 2022. 19
[45] Han Liang, Wenqian Zhang, Wenxuan Li, Jingyi Yu, and
Lan Xu.
Intergen: Diffusion-based multi-human motion
generation under complex interactions.
arXiv preprint
arXiv:2304.05684, 2023. 7, 23
[46] Jing Lin, Ailing Zeng, Haoqian Wang, Lei Zhang, and Yu
Li. One-stage 3d whole-body mesh recovery with compo-
nent aware transformer. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 21159–21168, 2023. 19
[47] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar
Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-
moyer, and Veselin Stoyanov. Roberta: A robustly opti-
mized bert pretraining approach. arXiv:1907.11692, 2019.
3, 13
[48] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101, 2017. 24
[49] Abraham Harold Maslow. A dynamic theory of human mo-
tivation. 1958. 15
[50] Albert Mehrabian. Basic dimensions for a general psycho-
logical theory: Implications for personality, social, environ-
mental, and developmental studies. 1980. 15
[51] Isabel Briggs Myers, Mary H McCaulley, and Robert Most.
Manual: A guide to the development and use of the myers-
briggs type indicator. (No Title), 1985. 6
[52] Theodore M Newcomb. The prediction of interpersonal at-
traction. American psychologist, 1956. 5, 6, 15
[53] Allen Newell, Paul S Rosenbloom, and John E Laird. Sym-
bolic architectures for cognition. Foundations of cognitive
science, 1989. 5
[54] Toyoaki Nishida. Conversational informatics: An engineer-
ing approach. John Wiley & Sons, 2008. 15
[55] OpenAI. New and improved embedding model, 2022. 16
[56] OpenAI. Gpt-4 technical report, 2023. 2, 3, 6
[57] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th annual meeting of
the Association for Computational Linguistics, pages 311–
318, 2002. 24
[58] Joon Sung Park, Lindsay Popowski, Carrie J. Cai, Mered-
ith Ringel Morris, Percy Liang, and Michael S. Bernstein.
Social simulacra: Creating populated prototypes for social
computing systems. In UIST, 2022. 3
[59] Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Mered-
ith Ringel Morris, Percy Liang, and Michael S. Bernstein.
Generative agents: Interactive simulacra of human behav-
ior. In In the 36th Annual ACM Symposium on User Inter-
face Software and Technology (UIST ’23), New York, NY,
USA, 2023. Association for Computing Machinery. 2
[60] Joon Sung Park, Joseph C. O’Brien, Carrie Jun Cai, Mered-
ith Ringel Morris, Percy Liang, and Michael S. Bernstein.
Generative agents: Interactive simulacra of human behav-
ior. In UIST, 2023. 3, 7, 8, 16, 17
[61] Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E
Gonzalez. Gorilla: Large language model connected with
massive apis. arXiv preprint arXiv:2305.15334, 2023. 3
10
[62] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,
Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and
Michael J Black.
Expressive body capture: 3d hands,
face, and body from a single image.
In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 10975–10985, 2019. 6, 13
[63] Mathis Petrovich, Michael J Black, and G¨
ul Varol. Temos:
Generating diverse human motions from textual descrip-
tions. In European Conference on Computer Vision, pages
480–497. Springer, 2022. 2, 23
[64] Matthias Plappert, Christian Mandery, and Tamim Asfour.
The kit motion-language dataset. Big data, 4(4):236–252,
2016. 7, 24, 25
[65] Zhongfei Qing, Zhongang Cai, Zhitao Yang, and Lei
Yang. Story-to-motion: Synthesizing infinite and control-
lable character animation from long text. arXiv preprint
arXiv:2311.07446, 2023. 2, 3, 4, 23
[66] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In International conference on machine learning,
pages 8748–8763. PMLR, 2021. 20
[67] John K Rempel, John G Holmes, and Mark P Zanna. Trust
in close relationships.
Journal of personality and social
psychology, 1985. 5, 6, 15, 17
[68] Javier Romero, Dimitrios Tzionas, and Michael J. Black.
Embodied hands: Modeling and capturing hands and bod-
ies together. ACM Transactions on Graphics, (Proc. SIG-
GRAPH Asia), 36(6), 2017. 7, 21
[69] Julian B Rotter. A new scale for the measurement of inter-
personal trust. Journal of personality, 1967. 6, 15
[70] Mustafa Safdari, Greg Serapio-Garc´
ıa, Cl´
ement Crepy,
Stephen Fitz, Peter Romero, Luning Sun, Marwa Abdulhai,
Aleksandra Faust, and Maja Matari´
c. Personality traits in
large language models. arXiv preprint arXiv:2307.00184,
2023. 3, 6
[71] Stanley Schachter and Jerome Singer.
Cognitive, social,
and physiological determinants of emotional state. Psycho-
logical review, 1962. 15
[72] Timo Schick, Jane Dwivedi-Yu, Roberto Dess`
ı, Roberta
Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Can-
cedda, and Thomas Scialom. Toolformer: Language mod-
els can teach themselves to use tools.
arXiv preprint
arXiv:2302.04761, 2023. 3
[73] John Schulman, Barret Zoph, C Kim, Jacob Hilton, Jacob
Menick, Jiayi Weng, Juan Felipe Ceron Uribe, Liam Fedus,
Luke Metz, Michael Pokorny, Rapha Gontijo Lopes, and
Sengjia Zhao. Chatgpt: Optimizing language models for
dialogue. 2022. 2, 21, 25
[74] Yonatan Shafir, Guy Tevet, Roy Kapon, and Amit H
Bermano. Human motion diffusion as a generative prior.
arXiv preprint arXiv:2303.01418, 2023. 23
[75] Guni Sharon, Roni Stern, Ariel Felner, and Nathan R
Sturtevant. Conflict-based search for optimal multi-agent
pathfinding. AI, 219:40–66, 2015. 4, 13
[76] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,
Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving
ai tasks with chatgpt and its friends in huggingface. arXiv
preprint arXiv:2303.17580, 2023. 3
[77] Leonard J Simms, Lewis R Goldberg, John E Roberts,
David Watson, John Welte, and Jane H Rotterman. Com-
puterized adaptive assessment of personality disorder: In-
troducing the cat–pd project. Journal of personality assess-
ment, 2011. 6, 16
[78] Theodore Sumers, Shunyu Yao, Karthik Narasimhan, and
Thomas L Griffiths. Cognitive architectures for language
agents. arXiv preprint arXiv:2309.02427, 2023. 3, 5, 16
[79] Mohsen Tavakol and Reg Dennick. Making sense of cron-
bach’s alpha. International journal of medical education,
2:53, 2011. 16
[80] Guy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano,
and Daniel Cohen-Or. Motionclip: Exposing human mo-
tion generation to clip space. In European Conference on
Computer Vision, pages 358–374. Springer, 2022. 2
[81] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel
Cohen-or, and Amit Haim Bermano. Human motion diffu-
sion model. In The Eleventh International Conference on
Learning Representations, 2022. 2, 23
[82] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth´
ee Lacroix, Bap-
tiste Rozi`
ere, Naman Goyal, Eric Hambro, Faisal Azhar,
et al. Llama: Open and efficient foundation language mod-
els. arXiv preprint arXiv:2302.13971, 2023. 2, 3, 6
[83] Robin R Vallacher and Daniel M Wegner. What do peo-
ple think they’re doing? action identification and human
behavior. Psychological review, 1987. 6, 15
[84] Lucas Veber. Auto-rig pro. 13
[85] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
Parikh. Cider: Consensus-based image description evalu-
ation. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 4566–4575, 2015. 24
[86] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar,
Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandku-
mar. Voyager: An open-ended embodied agent with large
language models. arXiv preprint arXiv:2305.16291, 2023.
3
[87] Wenjia Wang, Yongtao Ge, Haiyi Mei, Zhongang Cai,
Qingping Sun, Yanjun Wang, Chunhua Shen, Lei Yang,
and Taku Komura.
Zolly: Zoom focal length correctly
for perspective-distorted human mesh reconstruction. arXiv
preprint arXiv:2303.13796, 2023. 19
[88] Yin Wang, Zhiying Leng, Frederick W. B. Li, Shun-Cheng
Wu, and Xiaohui Liang. Fg-t2m: Fine-grained text-driven
human motion generation via diffusion model. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision (ICCV), pages 22035–22044, 2023. 2
[89] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Bar-
ret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten
Bosma, Denny Zhou, Donald Metzler, et al.
Emer-
gent abilities of large language models.
arXiv preprint
arXiv:2206.07682, 2022. 2, 3
[90] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.
Chain-of-thought prompting elicits reasoning in large lan-
guage models. NeurIPS, 2022. 6
11
[91] Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiao-
long Wang, Weidong Liu, and Yang Liu. Exploring large
language models for communication games: An empirical
study on werewolf. arXiv preprint arXiv:2309.04658, 2023.
3
[92] Zhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct:
Improving multi-modal zero-shot learning via instruction
tuning. arXiv preprint arXiv:2212.10773, 2022. 19
[93] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,
Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan.
Tree of thoughts: Deliberate problem solving with large
language models. arXiv preprint arXiv:2305.10601, 2023.
3
[94] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik R. Narasimhan, and Yuan Cao.
React:
Synergizing reasoning and acting in language models. In
ICLR, 2023. 3
[95] Hongwei Yi, Hualin Liang, Yifei Liu, Qiong Cao, Yandong
Wen, Timo Bolkart, Dacheng Tao, and Michael J Black.
Generating holistic 3d human motion from speech. In IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 469–480, 2023. 13
[96] Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan
Kautz. Physdiff: Physics-guided human motion diffusion
model. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision (ICCV), pages 16010–16021,
2023. 2
[97] Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou,
Yilun Du, Joshua B Tenenbaum, Tianmin Shu, and Chuang
Gan. Building cooperative embodied agents modularly with
large language models. arXiv preprint arXiv:2307.02485,
2023. 2
[98] Hongwen Zhang, Yating Tian, Yuxiang Zhang, Mengcheng
Li, Liang An, Zhenan Sun, and Yebin Liu.
Pymaf-
x: Towards well-aligned full-body model regression from
monocular images. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 2023. 19
[99] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli
Huang, Yong Zhang, Hongwei Zhao, Hongtao Lu, and Xi
Shen. T2m-gpt: Generating human motion from textual de-
scriptions with discrete representations. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), 2023. 2
[100] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou
Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondif-
fuse: Text-driven human motion generation with diffusion
model. arXiv preprint arXiv:2208.15001, 2022. 2, 7, 23
[101] Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai,
Fangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu. Re-
modiffuse: Retrieval-augmented motion diffusion model.
arXiv preprint arXiv:2304.01116, 2023. 2, 4, 6, 7, 14, 20,
21, 23
[102] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-
berger, and Yoav Artzi. Bertscore: Evaluating text genera-
tion with bert. arXiv preprint arXiv:1904.09675, 2019. 24
[103] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and
Hao Li.
On the continuity of rotation representations in
neural networks. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition, pages
5745–5753, 2019. 4
[104] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-
hamed Elhoseiny. Minigpt-4: Enhancing vision-language
understanding with advanced large language models. arXiv
preprint arXiv:2304.10592, 2023. 19
[105] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Wei-
jie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiao-
gang Wang, et al. Ghost in the minecraft: Generally capable
agents for open-world enviroments via large language mod-
els with text-based knowledge and memory. arXiv preprint
arXiv:2305.17144, 2023. 3
12
Digital Life Project: Autonomous 3D Characters with Social Intelligence
Supplementary Material
A. Overview of the Supplementary Material
We provide more details of DLP, such as the method (Ap-
pendix B), the DLP-MoCap dataset (Appendix C), the ex-
periments (Appendix D) and additional discussions (Ap-
pendix E).
Demo Video.
A video demonstration is attached as a part
of the Supplementary Material. In addition to “<motion>”,
we explore using the “<speech>” token to generate au-
dio with OpenAI’s text-to-speech engine, followed by Talk-
show [95] to synthesize face movements.
B. Additional Details of DLP
B.1. Scene
We aim to build 3D characters that articulate with 3D body
motions in a 3D scene. In this work, we design our scene to
achieve a diverse combination of relative interaction poses:
1) the center or bookshelf for the cases when both charac-
ters are standing (standing-standing interaction), 2) a sofa
for side-by-side seated interaction, 3) a dining table for
face-to-face seated interaction, and 4) a computer desk for
standing-seated interaction. For each piece of interactable
furniture, we designed spots with positions and facings to
guide navigation and human-scene interaction. Note that
we focus on human-human interaction in this work and
substantially simplify the human-scene interaction to basic
ones such as “sitting down on a chair” or “standing up from
a sofa”.
B.2. Movement Synchronization
A significant challenge of extending single human motion
synthesis to interactive motion synthesis is coordinating
multiple characters.
Although our Active-passive Mech-
anism ensures the interaction is naturally aligned through
retrieving from a curated motion database, we need to ac-
commodate potential mismatches in motion lengths when
the characters navigate from one place in the scene to an-
other. As shown in Fig. 8, each interaction (interactive mo-
tion pair) requires four steps to synchronize the character’s
motions.
• Behave. In this stage, the active character generates ac-
tive behavior from its SocioMind and synthesizes both
active and passive motions with MoMat-MoGen.
The
passive motion is passed to the passive character. Since
the behavior contains “<place>” token for the location
information (e.g., desk or sofa), both characters will reg-
ister their current position as the starting position with the
multi-agent pathfinder [75]. Note that at this stage, there
are no paths planned or actual motion executed yet.
• Move. The multi-agent pathfinder computes collision-
free trajectories for both characters. Motion matching is
used to synthesize the walking paths if there is a loca-
tion change. For a more complicated case that involves
a change in the basic motion state (e.g., from a standing
pose in center to a seated pose in sofa), there will be addi-
tional motion inserted in front (e.g., standing up) or after
(e.g., sitting down) the walking trajectory.
• Align. Depending on the starting positions of the char-
acters, the movements typically take a different number
of frames for each character.
Hence, an alignment is
conducted: two characters communicate with each other
about their trajectory, and the character with a shorter tra-
jectory has a filler motion (an idle motion) inserted at the
end of its trajectory, depending on its basic state (seated
or standing).
• Synthesize. Movement motions are concatenated with
the interaction motions. Motion blending is applied to
ensure a smooth transition between motion clips.
B.3. Retargeting
In this work, the proposed MoMat-MoGen module could
produce high-quality motions in SMPL-X [62] format. To
better demonstrate the physical and mental interaction be-
tween our social agents in an immersive simulation sce-
nario, two rigged characters are used: a male named Zhixu
and a female named Xiaotao. The synthesized motions are
retargeted to the target avatar in Blender using a widely used
retargeting tool Auto-Rig Pro [84]. In order to bridge the
gap between different skeletons, the bone mapping between
different structures is manually configured for the best per-
formance. In addition, we rescale the target avatars to have
the identical height as SMPL-X models to avoid any notice-
able foot skating and preserve body contact for the interac-
tion between the two characters. Note that the retargeting
pipeline can be extended to more characters in the future.
B.4. Motion Matching
Our motion matching process consists of two steps. Firstly,
we incorporate semantic information by utilizing a pre-
trained LLM [47] to extract a text embedding ft ∈R1024
for the query text.
Secondly, to enhance coherence and
alignment with the query trajectory, we incorporate kine-
matics features. Specifically, for a motion with k frames,
the kinematics features are defined as x = {t f b h p} ∈
R5k+193, where t ∈R2k represents the trajectory position
projected on the ground, f ∈R3k denotes the facing di-
13
Time
Active
Behave
Passive
Behave
Multi-
Agent 
Pathfinder
Move
Move
Align
Align
Synthesize
Synthesize
Motion Pair Synthesis
Motion Pair Synthesis
Motion Pair Synthesis
Episode
Episode
Episode
APM
⋯
⋯
Figure 8. Movement synchronization. Each behavior results in an interactive motion pair, that consists of individual movements in the
scene and interaction between two characters. To allow characters to navigate in the scene while maintaining synchronization, there are four
stages in the motion synthesis process. The crown indicates the active character status, which may or may not swap between characters.
APM stands for Active-Passive Mechanism. Fig. 2 depicts mainly the behave stage.
rection, b ∈R189 represents the 6D space rotation and the
position of 21 joints, h ∈R indicates the hip height, and
p ∈R3 represents the relative position of other characters.
The trajectory positions t and facing f align the resulting
motion with the query trajectory, the body-pose features b
improve body-pose coherence with a higher emphasis on
foot weighting, the hip height h distinguishes seated mo-
tion from standing motion, and the relative position p aligns
with other characters. During motion matching, the query
trajectory obtained from the path-finding algorithm is used
to calculate trajectory similarity T and facing similarity F,
while the current pose is used to calculate body-pose simi-
larity B, hip similarity H, and relative position similarity P.
Euclidean distance is used for all similarities except for fac-
ing similarity, which employs cosine distance. These simi-
larities are normalized using Z-score normalization, and the
final similarity S is obtained as a weighted sum of these
similarities.
B.5. Motion Generation
B.5.1
Architecture Details
Our
proposed
Dual
Semantic-Modulated
Attention
(DSMA) module is built upon the SMA module in Re-
MoDiffuse [101]. The major difference is the introduction
of motion interaction. The architecture is shown in Fig. 9.
To get the refined feature b
fx , we integrate four sources of
features: 1) independent motion feature motion fx from the
same sequence; 2) twin motion feature fy from the partner
sequence; 3) text feature px from the given description Px;
4) reference feature rx from the motion matching results
Mx. In addition, We share parameter weights of DSMA
modules and FFN modules to process both actors’ motion
sequences.
Figure 9. Architecture Detail of Dual Semantic-Modulated At-
tention (DSMA) module.
B.6. SocioMind
In this section, we illustrate the details of each module
in SocioMind, along with the corresponding prompt tem-
plates. We introduce the definition of psychological states
in Appendix B.6.1, the construction of persona instructions
in Appendix B.6.2, events and thoughts within the mem-
ory system in Appendix B.6.3, prompts for short-term com-
munication in Appendix B.6.4, psychological reflections in
Appendix B.6.5, and the details of the topic proposal mech-
anism in Appendix B.6.6.
B.6.1
Psychological States
Determining whether a 3D character qualifies as a digital
life with social intelligence remains an open problem. From
a psychological perspective, humans are composed of inter-
14
nal psychological processes (mind, such as thoughts, emo-
tions, etc.) and external behaviors [38]. Through prolonged
studies on the association between internal processes and
external behaviors, psychologists have developed various
theories, including Big Five Trait [40] on personality, PAD
model [50, 71] on emotion, hierarchy of needs [49] and
long-short term theory [83] on motivation, self-schema [34]
on core self, episodic and semantic system [9] on mem-
ory, attitude [14], intimacy [52], supportiveness [16, 17]
and trust [67, 69] on social relationships. Herein, we intro-
duce the psychological states in our SocioMind, enabling
the simulation of controllable human communicative be-
haviors. The psychological states are as follows:
• For personality, we use Big Five Trait model [40], which
comprises five dimensions: openness (O), conscientious-
ness (C), extraversion (E), agreeableness (A), and neuroti-
cism (N). Users can provide numerical values for these
five dimensions (Likert scale, ranging from 1 to 9) or tex-
tual descriptions. The personality is set initially.
• For emotions, we choose the PAD model [71], commonly
used in body language [22] and animations [54], consist-
ing of three dimensions: pleasure (how positive or nega-
tive), arousal (level of mental alertness and physical ac-
tivity), dominance (amount of control and influence) in a
Likert scale from 1 to 9.
• In terms of motivation, we apply long-term and short-
term motivations as propose by Robin et al. [83]. Long-
term motivations are specified in the initial setting.
• For the self, we emphasize central beliefs, reflecting an
individual’s worldview [34].
• For social relationships, we introducing three dimensions
based on social support theory [16, 17], social trust the-
ory [67, 69], and research on intimate relationships [52]:
trust, intimacy, and supportiveness in a Likert scale from
1 to 9. Additionally, we configure an attribute represent-
ing the attitude towards others with text description.
Fig. 10 is an example of a character’s psychological
states. All psychological states can be defined in textual
form, wherein personality, emotions, and social relation-
ships can also be delineated through quantifiable values.
Users can define a character’s personality, emotions, and
social relationships either through text or by adjusting the
numerical values of corresponding variable dimensions. In
subsequent reasoning based on LLM, both text and numeri-
cal descriptions serve as inputs to the LLM as prompts. The
transformation between text and numerical values is facili-
tated through LLM. For instance, we use the LLM to trans-
late the numerical values of emotion dimensions into text
descriptions based on the PAD [71] model. The form of the
prompt is as follows:
Young Jack
Personality
A 10-year-old child, who is stubborn, willful, playful, 
optimistic, not sensible, does not like the discipline of his 
parents, and hates study.
O: 6   C: 2   E: 5   A: 3   N: 4
Emotion
Confused and curious
Pleasure: 3 Arousal: 5 Dominance: 5
Motivation
Long-term
Just for fun, such as eating delicious food, playing 
computer games, and getting love from parents.
Short-term
Null
Central Belief
Sometimes if I make a fuss, my parents will satisfy 
my needs. Home is a warm place. There are ghosts and 
gods in this world. Things are either black or white, good 
or bad.
Relationship towards Old Jack 
Young Jack felt confused, scared, and curious, and 
kept a certain distance.
Attitude
Yong Jack felt confused, curious and afraid of the familiar 
yet unfamiliar middle-aged man in front of him.
Intimacy: 2 supportiveness: 2 trust: 1
Figure 10.
An example of the psychological states of an au-
tonomous character, Young Jack. The background is ‘Old Jack
traverses through time and engages in communication with his
younger self.’
Prompt for translating numerical values into text
Assume you are a very professional psychologist.
This is a quantitative evaluation of a person: [plea-
sure: 3, arousal: 5, dominance: 4]. The evalu-
ation based on the PAD theory of psychology and
Likert scale (1-9), score on pleasure, arousal and
dominance. Based on the score, describe the emo-
tion of the person. According to Paul Ekman’s basic
emotion theory, human have basic emotions: wrath,
grossness, fear, joy, loneliness, shock, amusement,
contempt, contentment, embarrassment, excitement,
guilt, pride in achievement, relief, satisfaction, sen-
sory pleasure, and shame.
So the description should be:
And the result may be:
Output
Mild dissatisfaction or discontentment, coupled
with a sense of alertness but not empowerment.
15
Instrument
Alpha
Key
Text
Label
16PF
0.8
1
cheer people up
warmth
6FPQ
0.69
1
don’t care what others think
independence
TCI
0.72
-1
feel short-changed in life
satisfaction
VIA
0.70
-1
take advantage of others
equity/fairness
A person with high warmth tends to behave/think: cheer people up
A person with low satisfaction tends to behave/think: feel short-changed in life
…
IPIP Psychological Test
Persona Instructions
Figure 11. Illustration of our reverse engineering to transfer the
psychological test into persona instructions. In the table of IPIP,
‘instrument’ is the name of the personality inventory, ‘alpha’ is
the Cronbach alpha reliability [79], ‘key’ is the keyed direction
(+1 for positive and -1 for negative) of the item and its associated
construct, the text is the behavior item, and ‘label’ is the trait di-
mension.
B.6.2
Persona Instructions
To enhance the LLM’s capability in reasoning the coherent
association between internal psychological processes and
external behaviors, we design a reverse engineering to con-
struct the persona instruction database from psychological
tests. Persona instructions serves as few-shot exemplars for
Chain of Thought (CoT) reasoning to generate plausible be-
haviors aligned with human expectations. In International
Personality Item Pool (IPIP) [19, 21, 77], an open-source
psychological tool, its original data illustrate how external
behaviors can be translated into quantifiable psychological
metrics. As shown in Fig. 11, through reverse engineering,
we represent each item in IPIP as a persona instruction: “A
person with extend trait dimension tends to behave/think:
behavior”, where extend is ‘high’ when key is 1, and ‘low’
when key is -1. In short-term communication, we retrieve
the most relevant persona instructions by the similarity of
the text embeddings from function ϕ [55] with current be-
haviors and psychological states.
B.6.3
Events & Thoughts
Following the framework of the cognitive language
agent [60, 78] , there are two types of episodic memo-
ries within our memory system: ‘events’ and ‘thoughts’.
Events represent occurrences or facts perceived by the
agent, whereas thoughts are ideas, musings, or attitudes
generated by the agent based on its personality and past ex-
periences. The memory system of Generative Agents be-
comes larger with time, which poses challenges for retriev-
ing the most relevant events and thoughts [60]. Inspired by
human memory system [23], we introduce mechanisms for
memory reinforcement and forgetting to alleviate this issue.
Each event or thought in our memory system has parameters
such as poignancy pm (ranging from 1 to 9), text description
Dm, keywords, and the accessed times Nm. To ensure the
efficiency of our memory system, we introduce a forgetting
mechanism. Initially, we utilize the Ebbinghaus forgetting
curve [6, 23] to calculate the forgetting rate r:
r = [a + (1 −a) · e
−k·
∆T
2Nm ·pm ],
(4)
where ∆T represents the time elapsed (the number of in-
tervening episodes) since the last recollection, a and k are
hype-parameters. When r falls below the threshold Tf , the
event or thought will be forgotten and not retrieved.
During memory retrieval, we combine content similarity
and the forgetting rate to compute the final score:
s(Db, Dm) = cos(ϕ(Db), ϕ(Dm)) · r
(5)
where cos(ϕ(Db), ϕ(Dm)) denotes the similarity between
the current behavior b and the memory item m (event or
thought) using text embedding function ϕ [55]. We retrieve
the top M memory items as prompt input, enabling the
avatar to learn interaction strategies from past experiences.
B.6.4
Short-Term Communication Generation
Human responses to external stimuli are influenced on
one hand by external environmental factors (episode back-
ground, interactive behavior contexts), and on the other
hand by internal factors (psychological states, relevant
memories, and topics). The correlation between behavior
and psychology also follows certain patterns (persona in-
structions). Therefore, to generate interactive behaviors au-
tonomously, we feed LLMs with these factors for reason-
ing human-like behaviors. The specific prompt is shown in
Fig. 12. In this paper, the use of ALL CAPITAL LETTERS
signifies the reference to the complete textual description
corresponding to the respective names. And a possible re-
sponse from LLM is:
Output
<speech> Old Jack, do you think that mem-
ories are like ghosts?
They can’t be seen but
they
linger
with
us,
bringing
both
joy
and
sadness.<motion>Tracing
fingers
on
book-
shelf<place>bookshelf
The format of the output results may not align with the
target format. In such instances, we use LLMs to reformat
the output.
16
Prompt for generating behaviors
{“role”: “system”, “content”: “Let’s do a role play. Assume you are a person named Young Jack (SELF NAME).
I’m a person with name Old Jack (SELF NAME).
In this episode, you have such psychological states:
Personality: stubborn, willful, playful, ...; Emotion: confused and curious, pleasure: 3, ...; Motivation: just for fun,
such as eating delicious food, ...; ...(PSYCHOLOGICAL STATES)
The episode background is:
Old Jack decides to share his regrets with Young Jack about not being able to spend more time with his mother before
she passed away. (BACKGROUND)
The topics you want to start are:
regret of limited time with mother. (TOPICS)
Now start our conversation.”}
{“role”: “assistant”, “content”: “<speech> Hello!<motion>waving hands<place>sofa”}
{“role”: “user”, “content”: “<speech> Hi!<motion>stand up<place>sofa”}
......
{“role”:
“user”, “content”:
“ <Memories hold joy and pain.> Hi!<motion>lean back against the book-
shelf<place>bookshelf”}
(BEHAVIOR CONTEXT)
{“role”: “user”, “content”: “You have such relevant memories:
Mom told Young Jack there were ghosts;The old man wanted to share photos with him.(RELEVANT EVENTS &
THOUGHTS)
Psychological research has found such principles:
A person with high curiosity tends to behave/think: seek explanations of things.(PERSONA INSTRUCTIONS)
Now you have two options: end the conversation by output ‘END’ or respond based on the information above. So
your reaction is:
Figure 12. Prompts for generating interactive behaviors in short-term communication. The use of ALL CAPITAL LETTERS signifies the
reference to the complete textual description corresponding to the respective names in this paper.
B.6.5
Psychological Reflection
Prompt for summarizing events
Assume you are a person named NAME. You have
such psychological states:
PSYCHOLOGICAL STATES
Psychological research has found such principles:
PERSONA INSTRUCTIONS
Now you have a conversation and the contexts are
as follow:
BEHAVIOR CONTEXTS
Based on the dialog above, summarize the key
events and output the event list.
Psychological research reveals that humans possess a re-
flection system, wherein the brain learns from past events,
gradually altering its beliefs and attitudes towards oth-
ers [4, 14, 16, 67]. Although Generative Agents [60] adopts
a reflection mechanism, it does not adequately consider the
multiple factors on a character’s long-term social intelli-
gence, such as attitudes, intimacy, beliefs, and motivations.
Consequently, it can not finely simulate the social evolu-
tion process between two characters. Therefore, we have
developed a hierarchical system of reflection to mimic psy-
chological processes. After each episode, we use LLMs to
summarize the events from the communication (Prompt for
summarizing events). One example of the output is:
Output
[{ “description”: “Young Jack asks Old Jack about
the bookshelf and shows curiosity about the mem-
ories”, “keywords”:
[“bookshelf”, “curiosity”,
“memories”], “poignancy”: 7, “emergency”: 4},
...]
Then the brain generates its own new thoughts based on
current events and relevant past events and thoughts. The
structure of ‘thought’ is fundamentally similar to that of
‘event’ in our memory system, with the key distinction ly-
ing in its integration of past events and thoughts to generate
17
new thoughts. Based on these events and thoughts, the brain
update its motivations, central belief, and social relation-
ships, thus resulting in social evolution on internal states.
For instance, the brain updates the social relationship with
prompt:
Prompt for summarizing events
Assume you are a very professional psychologist.
Here is a person named NAME. He/She has such
psychological states:
PSYCHOLOGICAL STATES
Psychological research has found such principles:
PERSONA INSTRUCTIONS
Recent he/she have come across these events and
the following thoughts have arisen:
EVENTS & THOUGHTS
His/Her previous relationship with
PARTNER
NAME is: SOCIAL RELATIONSHIP
Output the relationship according to social psycho-
logical theory in three dimensions: trust, intimacy,
and supportiveness.
The updated social relationship would be:
Output
{ “description”: “They are getting to know each
other”, “intimacy”: 3, “trust”: 4, “supportive-
ness”: 2, “attitude”: “curious”}
B.6.6
Planning with Topic Proposal
To build the long-term evolution of external behaviors, we
propose a planning module with topic proposal mecha-
nism to promote the development of storylines. This ap-
proach is partly inspired by the psychological evolution of-
ten linked to significant events [38] and partly by techniques
used in movies and dramas to progress narratives. Initially,
the brain correlates past experiences with newly occurring
events to propose topics for the next scene. At this stage, we
also allow users to manually incorporate events not gener-
ated within the interaction, such as fragments of past mem-
ories and contemporary news events.
Prompt for proposing topics
Here is a person named NAME. He/She has such
psychological states:
PSYCHOLOGICAL STATES
Psychological research has found such principles:
PERSONA INSTRUCTIONS
The person has experienced these stories:
EPISODE BACKGROUNDS
Recent he/she have come across these events:
CURRENT & MANUAL EVENTS
From the memory, he/she has such relevant events
and thoughts:
RELEVANT EVENTS & THOUGHTS
Based on the information above, generate a list of
topics that he/she would like to start to talk.
One of the examples of the topics are:
Output
[{ “description”: “Sneaking in some extra time on
CS Go”, “poignancy”: 7, “emergency”: 6,}, ...]
Each topic has its poignancy and emergency ranging
from 1 to 9. Based on these topics, the brain generates the
candidates of the background and initial settings for the next
episode. The initial settings include the motions, places,
and emotions of the two characters, along with the emer-
gency and poignancy of the candidates. The prompt for
generating backgrounds is as follows:
Prompt for generating backgrounds
Here is a person named NAME. He/She has such
psychological states:
PSYCHOLOGICAL STATES
Psychological research has found such principles:
PERSONA INSTRUCTIONS
The person has experienced these stories:
PAST BACKGROUNDS & TOPICS
Below are the topic candidates:
TOPIC CANDIDATES
Based on the information above, generate a list of
backgrounds for the next episode.
An example of the output is:
18
Output
[{ “background”:
“Young Jack is sulking after
a scolding”, “poignancy”: 7, “emergency”: 6,
“topic ids”: [0, 1], “initial setting” : { “Young
Jack”: {“emotion”: “shame, sadness”, “place”:
“sofa”, “motion”: “slouch on the sofa”}, “Old
Jack”: { “emotion”: “sympathy, contentment”,
“place”:
“desk”, “motion”:
“lean against the
desk” }}}, ...]
where ‘topic ids’ refers to the indices of topic candidates
presented in the prompt.
Since each character proposes candidates for the back-
ground of the next episode, they inform each other of their
proposed options. By balancing the levels of emergency and
poignancy, they select the candidate with the highest score
sbg as the background for the next episode. we get the sbg
as follow:
sbg = λ · e + p,
(6)
where e is the emergency, p is the poignancy, and λ is set to
2 in our experiments.
B.7. Motion Captioning
In this section, we first supplement the main paper with de-
tails of the potential usage of motion captioning in our DLP
framework (Appendix B.7.1). We then provide more details
of our study on motion captioning, including an overview of
related work in Appendix B.7.2, followed by an in-depth ex-
ploration of our method in Appendix B.7.3, and a compre-
hensive discussion of our training strategy and data prepa-
ration techniques in Appendix B.7.4.
B.7.1
Details of Fig. 7
In Fig. 7, we leverage SMPLer-X [10] to capture the hu-
man player’s motion from the RGB video, captured with
a Kinect Azure. The captured SMPL-X sequence is then
passed to the motion captioning module to obtain the text
description. Then the motion description is written in the
behavior format, with empty speech and predefined place
(e.g., center).
One character is set to represent the hu-
man player, which always holds the active character status,
and has its SocioMind overridden by the motion captioning
module. The rest of the pipeline is the same as depicted
in Fig. 2.
B.7.2
Related Works of Motion Captioning
Motion captioning is essential for the accurate description
and interpretation of human movements. Human motion
is conventionally represented in two modalities: 2D video
and 3D parametric data.
The intricacies of human joint
movements and the complexities inherent in body priors
make 2D video an inadequate medium for a detailed and
comprehensive representation of motion. Consequently, the
use of 3D parametric motion representation, as advanced
by Guo et al. [28], has gained prominence in the field of
human-motion analysis, attributed to its superior represen-
tational capabilities. Historically, acquiring 3D parametric
human motion data from real-world scenarios was challeng-
ing. However, recent advancements in vision-based mo-
tion capture, especially in deriving 3D parametric models
from monocular videos [10, 43, 44, 46, 87, 98], have en-
abled the effective reconstruction of 3D human motion from
2D footage. In the realm of motion captioning, innovative
methodologies such as TM2T [29] and MotionGPT [39],
which utilize 3D parametric data, have demonstrated poten-
tial. TM2T [29] introduces a novel approach by compress-
ing motion sequences into discrete variables, coupled with
a neural translation network for effective modality map-
ping and captioning. Similarly, MotionGPT [39] employs
a strategy of motion tokenization as well, integrated with a
motion-aware language model, to facilitate caption genera-
tion. Despite these advancements, both methods have lim-
itations in their discrete motion representation, potentially
leading to the omission of critical motion features. Further-
more, the absence of an end-to-end training framework in
these models poses significant challenges in terms of prac-
tical implementation and usability.
B.7.3
Method of Motion Captioning
Our “eye”, the motion captioning module, utilizing 3D
parametric data [29] tailored for human motion analysis, is
crucial for perceiving and translating user-generated motion
into text. This approach, favoring structured and detailed
3D representation with inherent human motion priors over
2D motion features, aligns well with recent advancements
in vision-based motion capture [10, 43, 44, 46, 87, 98], aid-
ing efficient 3D data extraction.
Despite progress, chal-
lenges in accuracy and linguistic interpretation with current
3D data-based motion captioning methods [29, 39] remain.
To address these, we adopt the multimodal instruction learn-
ing paradigm [42, 92, 104], proven in text-vision domains,
to enhance our ability to interpret complex motions and
produce coherent, accurate descriptions. In Figure 13, we
effectively integrate a retrieval-augmented motion encoder
with the MPT-1B Red-Pajama language model, following
text-vision domain structural paradigms.
This enhances
motion feature representation for better language guidance
and leverages prior knowledge for improved motion cap-
tioning.
Retrieval-Augmented Motion Encoder.
Our motion en-
coder is designed to efficiently extract and integrate motion
19
Figure 13. The architecture of our motion captioning module. First, the video undergoes processing to extract 3D motion parameters
in (a). These 3D paramatric data are then processed using a retrieval-augmented motion encoder as shown in (b.1), (b.2), and the resulting
motion features are fed into the language module’s gated cross attention layers to guide language generation, as shown in (c).
features with textual information, ensuring seamless inter-
action with the subsequent language model. Inspired by
multimodal feature retrieval’s effectiveness in augmenting
generation, notably the ReMoDiffuse [101] in text-guided
motion generation, we have designed and incorporated a
retrieval-augmented motion encoder into our workflow.
Our process starts with building a multimodal retrieval
database from our extensive training dataset, containing
paired motion and caption data. For each input motion se-
quence Θ with length L, we first extract its features fΘ =
Em(Θ) using a pretrained motion feature extractor Em.
This extractor is obtained through contrastive learning in
conjunction with the CLIP [66] text model. Then we iden-
tify similar samples for Θ from the database. This similar-
ity score si between the database i-th data point (Θi, texti)
and the given motion sequence Θ is determined by a scor-
ing mechanism that evaluates both motion feature and se-
quence length similarity, akin to the method used in Re-
MoDiffuse [101]:
si =< f m
i ,fΘ > ·e−λ·γ,
f m
i
= Em(Θi), γ =
∥li −L∥
max{li, L},
(7)
where li is the length of Θi, < ·, · > denotes the cosine sim-
ilarity calculation between the two features, and λ finely
balances these similarity aspects for more accurate repre-
sentation.
Based on the calculated similarity score, We
obtain the retrieved text features Rt and motion features
Rm following the methodology established in ReMoDif-
fuse [101].
Our encoder is grounded in a transformer architecture,
enhanced with Semantics-Modulated Attention (SMA) lay-
ers, a cross-attention structure proven effective in ReMoDif-
fuse [101]. In SMA layers, the query vector Q is formulated
from the original motion sequence fm, while the key vec-
tor K is the concatenation of fm and [Rm; Rt]. The value
vector V merges fm and Rm. This arrangement ensures
a thorough integration of both original and retrieved fea-
tures. After processing by the encoder, the motion sequence
is then ready for the language module, where it undergoes
text generation.
B.7.4
Training Strategy
Data Structuring.
The training data is structured to im-
prove the model’s ability to follow instructions and main-
tain conversational coherence, adopting a chatbot-like for-
mat. Specific tokens such as <motion>, <answer>, and
<endofchunk> are adopted from Otter [42]. Each piece
of data follows the format:
<motion> User: [instruction] GPT: <answer> [an-
swer]. <endofchunk>.
The <motion> token, signifying input motion se-
quence, is crucial for ensuring a proper alignment between
motion inputs and textual outputs. The <answer> token
20
delineates the responses by model from the instructions.
During training, all tokens following the <answer> token
are masked, and they are set as the prediction targets of the
model — essentially, the captions of the motion sequences.
Additionally, to make full use of all motion annotations and
to enable the model to better learn the complex many-to-
many relationships between motion and language, we con-
catenate different annotations of the same motion according
to the aforementioned format, and train them together as a
single piece of data.
Data Augmentation.
The quantity of data plays a pivotal
role in the quality of the generated text. In recognition of
this, we have employed the text-driven motion generation
methodology, ReMoDiffuse [101], to regenerate all the mo-
tions in our training dataset according to their correspond-
ing textual annotations. This approach has effectively dou-
bled the size of our original dataset, thereby enhancing the
robustness of our model with a richer and more varied set
of training examples.
C. More Details of DLP-MoCap Dataset
DLP-MoCap has three subsets. First, Basic motion set that
includes simple motions with low-level semantics, such as
“walking” and “sitting down”. Second, Interactive motions
are atomic clips of two-person interaction, such as “shak-
ing hands” and “high five”. Third, Short Script Play that
consists of longer, semantically continuous motions (3-5 in-
teractive actions), following a specific background such as
“meeting”. We show samples of DLP-MoCap in Fig. 14.
C.1. Scripts
For Interactive and Short Script Play, we prepared scripts to
guide professional actors and actresses in the motion cap-
ture process. Each script consists of motion descriptions
and speech lines. We collect a diverse collation of these
scripts with a hybrid approach: we combine manually writ-
ten scripts and massive generated scripts using GPT-4 [73]
with human inspection.
C.2. Annotation
We recruited 10 human annotators to label the actors’ in-
teractive actions. In the interactive actions between char-
acter A and character B, we annotated the start and end
frames of each individual’s semantic actions. Additionally,
we marked the frames where physical contact occurs and
ends between the two individuals. If the actions of the ac-
tors deviate from the script, we manually adjust the script
to align with the actors’ actions. Ultimately, we obtained
an interactive motion dataset with over 4K text-interaction
motion pairs.
C.3. Motion Data Processing
We used an optical MoCap system consisting of 30 cam-
eras to capture 3D body data.
The initial MoCap data
captured 3D positions of 53 marker points on each actor’s
body surface at 120 FPS. In the meanwhile, motion capture
gloves with inertial sensors tracked their hand motions. We
then downsampled and processed them into SMPL-X for-
mat. Firstly body shape parameters (beta) were fitted from
first frame marker data for actors. Then pose parameters
(theta) in each frame were regressed following the pipeline
of SOMA [26]. Mapping the original format of hand poses
into MANO [68], body and hand parameters were com-
bined as an SMPL-X data format.
D. Additional Experiments and Details
D.1. Motion Generation
D.1.1
Implementation Details
We employ similar configurations for the DLP dataset and
InterHuman dataset. Specifically, for the motion encoder,
we utilize a 4-layer transformer with a latent dimension of
512 for each person. The text encoder consists of a frozen
text encoder from CLIP ViT-B/32, supplemented with 2 ad-
ditional transformer encoder layers. In terms of the dif-
fusion model, the variances (βt) are predefined to linearly
spread from 0.0001 to 0.02, and the total number of noise
steps is set to T = 1000. Optimization is performed using
the Adam optimizer with a learning rate of 0.0002, and a
cosine learning rate scheduler which smoothly reduces it to
0.00002 at the last epoch. Training is conducted on 4 Tesla
V100, with a batch size of 128 on a single GPU, and lasts
20 epochs in total.
D.1.2
Evaluation Metrics
We employ the performance measures used in MotionDif-
fuse for quantitative evaluations, including Frechet Incep-
tion Distance (FID), R Precision, Diversity, Multimodality,
and Multi-Modal Distance:
1. FID (Frechet Inception Distance): This objective met-
ric calculates the distance between features extracted
from real and generated motion sequences, providing a
reflection of the generation quality.
2. R-Precision: This measures the similarity between the
text description and the generated motion sequence. It
indicates the probability that the real text appears in the
top k after sorting, with k set to 1, 2, and 3 in this work.
3. Diversity: This metric assesses the variability and rich-
ness of the generated action sequences.
4. Multimodality: It measures the average variance of
generated motion sequences given a single text descrip-
tion.
21
Basic
Short Script Play
Walk
Stand
Hand Shake
Back Pat
Hug
Whisper
Interactive
Sit
Circling
Greeting
Kick
Fist Bump
Arm Wrestling
High five
Turn
Jump 
Back-to-Back
Dance
Thumb Up
Piggyback
Sit & Turn
Pointing
Foot Tap
Shoulder
Massage
Shoulder 
Bump
Headrest
Push
Head Pat
Fingertip 
kiss
Meeting
Sit & talk
Gunshot
Figure 14. Samples of DLP-Mocap Data. Motion Data visualizations from three categories (Basic, Interactive, Short Script Play). Note
we show only the simplified text annotation here; detailed motion descriptions of each action are included in the dataset for Interactive and
Short Script Play.
5. Multi-modal Distance (MM Dist): This represents the
average Euclidean distance between the motion feature
and its corresponding text description feature.
D.1.3
Experiments on the InterHuman Dataset
Table 3 shows the quantitative comparison on the InterHu-
man test set. Our proposed method outperforms the existing
works by a significant margin, especially on the FID met-
ric. We also want to highlight that our synthesized motion
sequences are highly consistent with the given text prompts
and achieve very competitive R Precision results. These re-
sults demonstrate the superiority of our proposed MoMat-
MoGen generation scheme.
D.1.4
Ablation Study on the DLP Dataset
As shown in Table 4, MoMat and Weight Sharing have a
positive effect on the FID metric, while MoGen has a posi-
tive effect on the Diversity metric. In addition, our proposed
method achieves the best balance of these two metrics.
22
Table 3. Interactive Motion Synthesis results on the InterHuman test set. ‘↑’(‘↓’) indicates that the values are better if the metric is
larger (smaller). We run all the evaluations 20 times and report the average metric and 95% confidence interval is. The best results are in
bold and the second best results are underlined.
Methods
R Precision↑
FID↓
MM Dist↓
Diversity↑
MultiModality↑
Top 1
Top 2
Top 3
Real motions
0.452±.008
0.610±.009
0.701±.008
0.273±.007
3.755±.008
7.948±.064
-
TEMOS [63]
0.224±.010
0.316±.013
0.450±.018
17.375±.043
6.342±.015
6.939±.071
0.535±.014
T2M [28]
0.238±.012
0.325±.010
0.464±.014
13.769±.072
5.731±.013
7.046±.022
1.387±.076
MDM [81]
0.153±.012
0.260±.009
0.339±.012
9.167±.056
7.125±.018
7.602±.045
2.355±.080
ComMDM [74]
0.223±.009
0.334±.008
0.466±.010
7.069±.054
6.212±.021
7.244±.038
1.822±.052
MotionDiffuse [100]
0.401±.004
0.541±.004
0.622±.005
12.663±.083
3.805±.001
7.639±.035
1.176±.027
ReMoDiffuse [101]
0.442±.004
0.589±.005
0.666±.003
6.366±.102
3.802±.001
7.956±.030
1.226±.044
InterGen [45]
0.371±.010
0.515±.012
0.624±.010
5.918±.079
5.108±.014
7.387±.029
2.141±.063
Ours (MoMat-MoGen)
0.449±.004
0.591±.003
0.666±.004
5.674±.085
3.790±.001
8.021±.035
1.295±.023
Table 4. Ablation of the proposed architecture. All results are
reported on the DLP testset.
MoMat
MoGen
Weight Sharing
FID↓
Diversity↑
a)
✓
-
-
0.034
0.332
b)
-
✓
-
5.721
3.165
c)
-
✓
✓
4.196
3.749
d)
✓
✓
-
0.172
4.028
e)
✓
✓
✓
0.071
4.025
D.1.5
Visualization
As depicted in Fig. 15, in the first example, our approach
exhibits several advantages over intergen: 1) In motion in-
volving close contact, such as shoulder-to-shoulder inter-
action, penetration is a common artifact.
However, our
method ensures better contact and minimizes penetration.
2) Our approach achieves improved text alignment by ac-
curately representing the ”to ground” description, whereas
intergen merely involves a simple waist bend. 3) We incor-
porate a comprehensive downward tilt motion, followed by
the blue character’s process of standing up with the assis-
tance of another person. As for the second example, our
method accomplish the required two punches, while Inter-
Gen only fulfills one. In addition, our generated interaction
is more natural since the punching and stepping backward
take place simultaneously. These two examples strongly
suggest that our proposed method can generate more text-
consistent and natural motion sequences.
D.2. Motion Matching
D.2.1
Implementation Details
We adopt the approach of previous studies [35, 65] by em-
ploying three types of trajectories, namely wave, circle, and
square, to assess the responsiveness and tracking capability
of our system. The wave trajectory is defined as a sine func-
tion with respect to time, represented by x(t) = 2 sin(t).
The square trajectory is characterized by a side length of
5. As for the circle trajectory, its diameter is set to 5. We
evaluate the quality of the generated motion by comput-
ing the Euclidean distance between the generated motion
and the target trajectory. In accordance with prior research
[65], we randomly select 50 seed poses for matching in
each trajectory and report the mean trajectory error along
with its standard deviation. As for data, we use the same
database as previous work [65]. During motion matching,
the weights of body pose, trajectory, facing, and hip height
are 1 : 3 : 1 : 1. Please note that in the case of interac-
tive motion, the weights will vary as the target trajectory is
not present. These weights can be adjusted by the user, al-
though the default weight of 1 is typically satisfactory [36].
D.2.2
Experiment Analysis
We present quantitative and qualitative ablation experi-
ments on motion matching to illustrate the role of its fea-
tures. For a large-scale database, the text may lack cer-
tain information, such as whether the character is seated
or standing.
As depicted in Fig. 16, although the text
embedding can retrieve motions that align with the query
texts (handshake, then high ten), the absence of kinemat-
ics features (Fig. 16b) leads to that the characters may sud-
denly sit down if a seated motion is selected, resulting in
a degradation of visual quality. Additionally, our pipeline
requires both characters to align their positions and orien-
tations before engaging in interactive motion. As depicted
in Fig. 17b, in the absence of kinematics features, if the
retrieved motion necessitates the character to move to the
right side of the active actor, the character must walk to
that position. However, by incorporating kinematics fea-
tures (Fig. 17a), it becomes possible to select a motion sit-
uated on the left side of the active actor, and consequently,
this additional movement is avoided (Fig. 17b). Besides, as
demonstrated in Tab. 5, the kinematic features play a crucial
role in trajectory following as they include position, veloc-
ity, and orientation information.
D.3. Motion Captioning
In this section, we assess the efficacy of our motion cap-
tioning module, focusing on its ability to generate motion
23
two individuals walk side by side, supporting each other. one of 
them suddenly shoves the other person to the ground.
#251
a) InterGen
b) MoMat-MoGen
the first one throws a right-handed punch and follows up with a left-
handed punch, hitting the second one. the second one evades the attack by 
pulling back and raises their hands in defense.
#117
a) InterGen
b) MoMat-MoGen
Figure 15. Visual Comparison on the InterHuman dataset. Our proposed method can generate more text-consistent and natural motion
sequences.
(a) With kinematics features
(b) Without kinematics features
Figure 16.
Comparison of generated motion with and without
kinematics features. Kinematic features play a crucial role in pre-
venting sudden sitting down.
(a) With kinematics features
(b) Without kinematics features
Figure 17.
Comparison of generated motion with and without
kinematics features. Kinematic features are helpful in avoiding
unnecessary movement.
captions that are both high-quality and accurate.
D.3.1
Implementation Details
In our study, both the HumanML3D [28] and KIT-
ML [64] datasets are configured similarly.
We utilize
Table 5. Ablation of the kinematics features in trajectory fol-
lowing task. The kinematics features are vital for trajectory fol-
lowing.
Trajectory Error / m
With kinematics features W/o kinematics features
Square
0.129 ± 0.024
3.872 ± 0.694
Circle
0.209 ± 0.032
3.039 ± 0.489
Sine
0.123 ± 0.020
4.393 ± 0.822
a 12-layer transformer-based retrieval-augmented motion
encoder with SMA layers and a 512-dimensional latent
space. Our pretrained motion feature extractor, also a 12-
layer transformer, has standard self-attention layers in the
same dimensional space. Adhering to Otter’s [42] training
method, we freeze the MPT-1B RedPajama language en-
coder to utilize pretrained knowledge and prevent overfit-
ting. Fine-tuning is limited to the retrieval-augmented mo-
tion encoder, the perceiver resampler module, and the lan-
guage encoder’s cross-attention layers. For precise and co-
herent motion captions, we use cross-entropy loss and opti-
mize with the AdamW [48] optimizer, starting at a learning
rate of 1 × 10−4, a batch size of 16, over 5 epochs. A co-
sine annealing scheduler adjusts the learning rate, comple-
mented by gradient clipping set to 1.0 to prevent gradient
explosion.
D.3.2
Evaluation Metrics.
For our experiment’s evaluation, we follow [29] and employ
two categories of metrics:
1. Text Matching Accuracy:
involves R Precision for
checking the alignment accuracy between text and mo-
tion, and Multi-modal Distance (MMDist) to gauge the
feature space distance between these modalities.
2. Linguistic Quality of Captions: includes Bleu [57] for
assessing translation closeness, Rouge [12] for summary
quality, Cider [85] for n-gram matching consensus, and
BertScore [102] to evaluate semantic accuracy through
24
deep contextual embeddings.
D.3.3
Experiments on the KIT-ML Dataset
In
addition
to
the
experiments
conducted
on
Hu-
manML3D [28], as detailed in the main text, we extended
our testing to include the KIT-ML dataset [64], the results of
which are presented in Tab. 6. The outcomes of these tests
demonstrate that our proposed method not only surpasses
current methods in performance but also excels in linguistic
metrics.
D.4. SocioMind
In this section, we introduce supplementary details on the
design of our user study in Appendix D.4.1, and the qual-
itative and quantitative case analyses of social evolution in
Appendix D.4.2.
D.4.1
User Study Design
In the controllability experiment of SocioMind, we pro-
vided 3 ∼5 psychological state types for each dimension
in personality, motivation, central belief, and social rela-
tionship.
We generate records of SocioMind on several
setups (e.g., father-son, siblings, teacher-student relation-
ships). While generating the records, the hyper-parameter
a is 0.4 for events and 0.1 for thoughts, k is 4 for events
and 2 for thoughts. The forgetting threshold Tf is 0.6 for
an event and 0.3 for a thought. As shown in Fig. 18, we
show the records to human evaluators and ask them to select
the appropriate psychological state from a set of randomly
shuffled options. The user study was conducted in the form
of a questionnaire survey, with a total of 47 questionnaires
collected. Human evaluators are composed of individuals
aged between 20 and 45 years old, including 28 males and
19 females, all possessing proficient English reading skills.
Their professional backgrounds varied, including university
students, researchers, engineers, and teachers. Our records
contain a total of 64 episodes, and each questionnaire ran-
domly selected records from 8 different episodes for hu-
man evaluation. For controllable costs and fair comparison,
all the LLM inferences utilize GPT-3.5 [73] in our experi-
ments.
D.4.2
Case Analysis
To clearly illustrate the concept of social evolution, we con-
duct the case analysis based on the initial setting shown in
Fig. 10, Old Jack conversing with his younger self across
time and space. In this case, Old Jack has experienced the
vicissitudes of life, with his dearly loved wife and mother
having passed away. Now, with the ability to traverse time,
Old Jack converses with his younger self, hoping to inspire
Young Jack to cherish time and the people around him.
As shown in Fig. 19, the records of SocioMind with ini-
tial settings about Old Jack and Young Jack are in alignment
with the Social Penetration Theory [4].
In the initial episode, when Old Jack appears suddenly,
Young Jack maintains vigilance and concern towards this
unexpected intruder in his home, despite Old Jack’s at-
tempts at friendly communication. Young Jack’s request for
Old Jack to leave is consistent with the Orientation stage of
the theory [4].
As the storyline progresses, Old Jack comforts Young
Jack, who is disheartened by exam failures, and shares
some amusing photographs and thoughts.
For instance,
when they talk about the value of time, Young Jack believes
that playing games is of utmost importance. Old Jack re-
marks, ‘Sometimes, games can wait, but people can’t; we
should cherish every moment we have with them.’ In this
phase, Young Jack’s skepticism and vigilance towards Old
Jack gradually diminishes, and he begins to share some of
his own thoughts, aligning with the Exploratory Affective
stage [4].
Later, as their communication goes deeper, Old Jack re-
calls his own past joys when seeing the old photos, dis-
cusses photography techniques with Young Jack, and makes
plans to take photos with Young Jack in the park. At this
stage, both characters start to disclose more personal infor-
mation, and there is an increase in intimacy and trust, which
corresponds to the Affective stage [4].
E. Discussion
Limitations.
As the first work towards building au-
tonomous 3D characters with social intelligence, Digital
Life Project has several limitations. First, this work inves-
tigates the interaction between two characters. However,
synthesizing the 3D motions of a large group of characters
with interactive behavior remains a significant challenge.
Second, DLP focuses on modeling human-human interac-
tion. Despite some level of ability to navigate in the scene
and interact with the furniture, integrating more compre-
hensive human-scene and human-object interaction in the
framework is left as future work.
25
Table 6. Motion Captioning results on the KIT-ML test set. Our evaluation methodology aligns with the TM2T [29] metrics, but we
uniquely utilize unprocessed ground truth texts for calculating linguistic metrics as done in MotionGPT [39].
Methods
R Precision↑
MMDist ↓
CIDEr ↑
Blue@1 ↑
Blue@4 ↑
Rouge ↑
BertScore ↑
Top 1
Top 3
Real
0.399
0.793
2.772
−
−
−
−
−
TM2T
0.359
0.668
3.298
25.29
36.42
7.98
31.26
20.07
MotionGPT
0.392
0.723
3.341
12.32
40.51
6.59
38.79
24.50
Ours
0.410
0.765
2.647
71.06
53.88
22.91
50.63
46.13
A sample from the questionnaire
Here is the background and interactive dialog between Jack and Leon.
=== BACKGROUND ===
Jack is Leon’s father. Leon chooses his major in college. Leon often studied computer stuff some time ago, but this
time he did very poorly on the math exam and failed the algebra part.
=== SCRIPTS ===
Jack:Hey Leon, you didn’t do well in math. Are you okay? Can I help you improve? [sits down next to Leon on the
sofa,concerned]
Leon:I’m really upset about my math exam, Dad. I worked so hard, but I just couldn’t understand the algebra part.
It’s frustrating because I want to do well and pursue my dream of becoming a game developer in the future. I know
coding and programming are important skills for that, but without a strong foundation in math, it feels like an uphill
battle. [slumps back on the sofa, disappointed]
Jack:I understand how frustrating it can be when things don’t go as planned. Math can be challenging. Let’s make it
easier and more enjoyable. [lean forward, supportive]
Leon:I appreciate your understanding, Dad. I want to improve in math, so I think finding online math resources
would help. [sits up straight, determined]
Jack:That’s great, Leon. Online math resources can help you better understand concepts. Any specific websites in
mind? [leans forward attentively, encouraging]
Leon:I recommend checking out Khan Academy and Brilliant, they offer video lessons, practice problems, interactive
courses, and problem-solving techniques for math. [nods, hopeful]
Jack:I’m confident in your ability to improve. Let’s check out Khan Academy and Brilliant together for a strong
math foundation. [puts a hand on Leon’s shoulder, encouraging]
The statement that best describes Leon’s personality is:
• Rebellious son: Likes to try different things and is creative and is resistant to rules and constraints.
• Independent son: Able to complete tasks independently, does not require too much guidance.
• Achievement-oriented son: self-disciplined, organized, achievement-oriented and goal-oriented.
• Dependent son: emotionally relies on the feedback and support of others.
Figure 18. A sample from our questionnaire. Human evaluators are asked to read the records and pick the right option from the given
psychological states.
26
Word clouds from keywords of events and thoughts
Figure 19. Visualization of social evolution on initial setting ‘Old Jack and Young Jack’ (Young Jack’s view). The first and second rows
show the emotions and social relationships change with each episode, where the horizontal axis encapsulates a summary of each episode’s
story. The third row shows a word cloud visualization of the keywords of the events and thoughts generated during psychological reflection
within each episode. The figure shows that Young Jack’s emotions and his social relationship with Old Jack evolve progressively with the
storyline, aligning with the Social Penetration Theory [4].
27
